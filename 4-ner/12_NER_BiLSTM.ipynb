{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Lab 12\n",
    "## Bidirectional LSTM for Named Entity Recognition\n",
    "\n",
    "In this notebook we will train a Bi-LSTM on NER dataset to create the appropriate tags\n",
    "\n",
    "Take it easy and notice the structure of the dataset to ensure for each word if it has a tag or not\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FO4He6AuvCvD",
    "outputId": "ac5a9e00-f7fc-4d1a-9e1f-7bac95d15d81"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qT84LO_ryWGC",
    "outputId": "de1c6d69-2de2-4c0a-c384-2acd0273e6e5"
   },
   "outputs": [],
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6h9-CflyyWIf",
    "outputId": "f6d9704e-f315-4e0b-cfa5-54f49b8baac6"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import Model, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout, LSTM, TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import keras_nlp\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.nn import leaky_relu\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "TRACE = False\n",
    "embedding_dim = 100\n",
    "rnn_units = 128\n",
    "epochs=100\n",
    "buffer_size = 256\n",
    "max_len = 50\n",
    "# Batch size\n",
    "batch_size = 256\n",
    "min_count_words = 3\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config) \n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0oENn5_JyWLG",
    "outputId": "a9051800-174c-45ee-f52e-a023d8c65694"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f ner_dataset.csv ]; then\n",
    "  wget -O ner_dataset.csv https://www.dropbox.com/s/mbfv0x988mdj89h/ner_dataset.csv?dl=0\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkxCOJz_yWNu"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "18V5m92oyWQK",
    "outputId": "87e9ab70-6d92-4956-b033-df0d61b0cef6"
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"./ner_dataset.csv\",encoding=\"latin1\")\n",
    "data = None # Forward Fill NaN\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RzRPJPTyWS2",
    "outputId": "7cfb62fd-7a4f-4ca6-ecee-9a6652c0b07b"
   },
   "outputs": [],
   "source": [
    "print(\"Unique Words in corpus:\",data['Word'].nunique())\n",
    "print(\"Unique Tag in corpus:\",data['Tag'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYF9u8Dp13YW"
   },
   "outputs": [],
   "source": [
    "words = None # Get list of words\n",
    "words.append(\"ENDPAD\")\n",
    "num_words = len(words)\n",
    "tags = None # Get list of tags\n",
    "num_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XAv4P3G1-7D"
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "  def __init__(self,data):\n",
    "    self.n_sent = 1 #counter\n",
    "    self.data = data\n",
    "    agg_func = lambda s:[(w,p,t) for w,p,t in zip(s['Word'].tolist(),s['POS'].tolist(),s['Tag'].tolist())]\n",
    "    self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "    self.sentences = [s for s in self.grouped]\n",
    "\n",
    "\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tP7TzZ71-9d",
    "outputId": "3ea48cf6-d597-49cd-fd03-16b9bca4785f"
   },
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4IPXmOQ1_Ci"
   },
   "outputs": [],
   "source": [
    "word2idx =  {w : i+1 for i,w in enumerate(words)}\n",
    "tag2idx  =  {t : i for i,t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ox9i1Zc1_FN"
   },
   "outputs": [],
   "source": [
    "X = None # Construct tensor of X with the ids of words and pad to the right\n",
    "\n",
    "y = None # Construct tensor of y with the ids of tags and pad to the right\n",
    "y = [to_categorical(i, num_classes=num_tags) for i in y]\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1JGVp1x30AY"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape = (max_len,)))\n",
    "model.add() # Add embedding layer\n",
    "model.add()  # Add SpatialDropout\n",
    "model.add()  # Add a Bi-LSTM with 100 units and a recurrent dropout for forgetfulness\n",
    "model.add(TimeDistributed(Dense(num_tags,activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "877hAFmB4lJK",
    "outputId": "c560a9f2-ba13-454f-aeef-3e77407b4808"
   },
   "outputs": [],
   "source": [
    "# Compile the model, use  kullback leibler divergence as metric (we will talk why)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3-ZFBRX4soC",
    "outputId": "1b5776d6-23f8-49f1-e098-8ea35e0ad4ce"
   },
   "outputs": [],
   "source": [
    "# Train the model with early stopping if the validation kullback leibler metrics goes up, and return the best weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QF6I4FYU5LKf"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFgXTGOE5Mia"
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(0, x_test.shape[0])\n",
    "p = model.predict(np.array([x_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "\n",
    "y_true = np.argmax(np.array(y_test), axis=-1)[i]\n",
    "print(\"{:15}{:5}\\t {}\\n\".format(\"Word\",\"True\",\"Pred\"))\n",
    "print(\"-\"*30)\n",
    "for w,true,pred in zip(x_test[i], y_true, p[0]):\n",
    "  print(\"{:15}{:5}\\t{}\".format(words[w-1], tags[true],tags[pred]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCyQIGqG6alm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNkBFEVlpi8b2n3v9uiLtjR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
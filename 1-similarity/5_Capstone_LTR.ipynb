{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Learning to Rank\n",
    "## RankNet\n",
    "\n",
    "In this notebook we will train on random data the learning to rank model RankNet.\n",
    "\n",
    "The idea behind LTR is always to start with a dataset of some queries, their returned documents and the score of relevance. This relevance may be an *a posteriori* metric like number of clicks.\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind RankNet is to model the **joint probability** that `document i` comes before `document j` as the following:\n",
    "\n",
    "$P_{ij} = 1$ if $s_i > s_j$\n",
    "$P_{ij} = 0.5$ if $s_i = s_j$\n",
    "$P_{ij} = 0$ if $s_i < s_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for *every pair of inputs* we will calculate both outputs, substract them, pass a logistic function to model the probability:\n",
    "\n",
    "<img src=\"./ranknet.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bb6KgZcudKT",
    "outputId": "6a29417f-601e-45eb-909e-7245e035af69"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVyuAnDLyvBE",
    "outputId": "c5db7809-56f7-4e0b-f2c9-e08910a9c3f4"
   },
   "outputs": [],
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLsDbhgoqZyt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "import gensim\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Activation, Dense, Subtract\n",
    "from tensorflow.nn import leaky_relu\n",
    "\n",
    "TRACE = False\n",
    "embedding_dim = 100\n",
    "epochs=50\n",
    "batch_size = 50\n",
    "sample_queries = 20\n",
    "sample_results_dataset = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWTlcgNpqZyv"
   },
   "outputs": [],
   "source": [
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config) \n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVRPlgP2wcpc",
    "outputId": "2552b401-8ecd-42d3-95fb-71bece257cea"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "\n",
    "if [ ! -f yelp.csv ]; then\n",
    "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
    "fi\n",
    "if [ ! -f doc2vec_yelp_model ]; then\n",
    "  wget -O doc2vec_yelp_model https://www.dropbox.com/s/bibu9bashb0cd68/doc2vec_yelp_model?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_HceAKmwc3m",
    "outputId": "d28aa654-5009-40a5-957b-3b1719c76e8b"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ec69pypxwc6Z"
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"./doc2vec_yelp_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHwqq3Vdwc9k"
   },
   "outputs": [],
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "train_set_reviews = yelp.sample(n=sample_results_dataset).reset_index(drop=True)\n",
    "queries = yelp.text.sample(n=sample_queries).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz8kOxJRwdAm"
   },
   "outputs": [],
   "source": [
    "# Here results will be a tensor that for each query_id and revew_id it will hold the inferred vector of the review by the doc2vec model.\n",
    "# We will use it to create the pair of reviews (xi, xj) that will be the input of our model\n",
    "results = np.zeros((len(queries), len(train_set_reviews), 100))\n",
    "\n",
    "\n",
    "# The scores tensor will have for each query and review the similarity using the doc2vec model.\n",
    "# This similarity score we will use it later to get the pij using the formulas at the start, and that pij will be the rue values to predict\n",
    "scores = np.zeros((len(queries), len(train_set_reviews)))\n",
    "\n",
    "for q_ix, query in enumerate(queries):\n",
    "  for r_ix, review in enumerate(train_set_reviews):\n",
    "      #  FILL\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WodU_Ep_zXzY"
   },
   "outputs": [],
   "source": [
    "# put data into pairs\n",
    "xi = []\n",
    "xj = []\n",
    "pij = []\n",
    "pair_id = []\n",
    "pair_query_id = []\n",
    "\n",
    "for q_ix, query in enumerate(queries):\n",
    "    for pair_idx in combinations(enumerate(results[q_ix]), 2):\n",
    "        pair_query_id.append(query)\n",
    "        pair_id.append(pair_idx)\n",
    "        ix_i, document_i = pair_idx[0]\n",
    "        ix_j, document_j = pair_idx[1]\n",
    "        xi.append(document_i)\n",
    "        xj.append(document_j)\n",
    "\n",
    "        pij = None  # Find pij for each q_ix, pair_idx with the help of the scores matrix and the formula at the start\n",
    "        pij.append(_pij)\n",
    "\n",
    "xi = np.array(xi)\n",
    "xj = np.array(xj)\n",
    "pij = np.array(pij)\n",
    "pair_query_id = np.array(pair_query_id)\n",
    "del results\n",
    "del scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5i-IcbTQzX19"
   },
   "outputs": [],
   "source": [
    "# FILL\n",
    "\n",
    "# Split xi, xj, pij, and pair_id into train and test sets setting the kwarg stratify to be pair_query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH_8rmrfzX4X"
   },
   "outputs": [],
   "source": [
    "xi_train = tf.constant(xi_train)\n",
    "xi_test = tf.constant(xi_test)\n",
    "xj_train = tf.constant(xj_train)\n",
    "xj_test = tf.constant(xj_test)\n",
    "pij_train = tf.constant(pij_train)\n",
    "pij_test = tf.constant(pij_test)\n",
    "pair_id_train = pair_id_train\n",
    "pair_id_test = pair_id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP0hwcIxqZyz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Try to create a model with 2 dense layers with leaky_relu as activations. Then a linear dense function and a substract layer.\n",
    "\n",
    "# This time I will leave it blank, but in the parameter oij you should have the output of the substraction.\n",
    "\n",
    "\n",
    "# model architecture\n",
    "class RankNet(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # FILL\n",
    "\n",
    "    def call(self, inputs):\n",
    "        xi, xj = inputs\n",
    "        # FILL\n",
    "        output = layers.Activation('sigmoid')(oij)\n",
    "        return output\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = [Input(shape=(10)), Input(shape=(10))]\n",
    "        return Model(inputs=x, outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIWp7ZNtqZy2",
    "outputId": "dee9126c-f11e-4a91-fbb4-a5d39d540b6b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train model using compile with binary_crossentropy.\n",
    "ranknet = RankNet()\n",
    "\n",
    "ranknet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Train the model, pass as inputs [xi_train, xj_train] and pij_train.\n",
    "history = None # Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nElcBh2dB3If",
    "outputId": "dcd481f2-7d04-4c06-97c2-d98b406f02cf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "MQOC3drYqZy2",
    "outputId": "b8678027-abd7-4d58-c29b-c43816aa0a8f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function for plotting loss\n",
    "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.plot(train_metric,color='blue',label=metric_name)\n",
    "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "# plot loss history\n",
    "plot_metrics(history.history['loss'], history.history['val_loss'], \"Loss\", \"Loss\", ylim=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GY6qE2UpqZy3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test with a a new sample pair of docs to get their associated probability.\n",
    "\n",
    "new_doci = None\n",
    "new_docj = None\n",
    "inputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOdqELijBybJ",
    "outputId": "26ffcd99-a129-4600-85fd-f18716a12c5a"
   },
   "outputs": [],
   "source": [
    "ranknet(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9Sp4l4AByrh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
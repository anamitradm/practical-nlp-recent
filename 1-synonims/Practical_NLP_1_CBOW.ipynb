{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN1RRXInuWjPRtTj9s7toOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/practical-nlp/blob/main/1-synonims/Practical_NLP_1_CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jbFcxFZhG5K",
        "outputId": "6de00334-10c8-4eec-8309-46b3be907cb7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iklSJ4lqUQlT",
        "outputId": "326f9499-7640-48f1-b9ca-6c554cd3d3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from textblob import TextBlob, Word\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "\n",
        "TRACE = False\n",
        "embedding_dim = 50\n",
        "epochs=100\n",
        "batch_size = 500\n",
        "BATCH = True\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config) \n",
        "  K.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "tokenizer = lambda x: TextBlob(x).words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f yelp.csv ]; then\n",
        "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l13de14sclyD",
        "outputId": "bcc537cd-8071-4a0e-933a-9e9f68a9e10b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting get_data.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash get_data.sh"
      ],
      "metadata": {
        "id": "PvRXU9EMVJMp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = './yelp.csv'\n",
        "yelp = pd.read_csv(path)\n",
        "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
        "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
        "X = yelp_best_worst.text\n",
        "y = yelp_best_worst.stars.map({1:0, 5:1})"
      ],
      "metadata": {
        "id": "QAWXcLEieD4E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [sentence for sentence in X.values if type(sentence) == str and len(TextBlob(sentence).words) > 3]\n"
      ],
      "metadata": {
        "id": "ljgSnKkzeM4-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "tokenized_corpus = tokenizer.texts_to_sequences(corpus)\n",
        "nb_samples = sum(len(s) for s in tokenized_corpus)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "dUlTe1xsgi51"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'First 5 corpus items are {tokenized_corpus[:5]}')\n",
        "print(f'Length of corpus is {len(tokenized_corpus)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfR6qIZZhIHd",
        "outputId": "3316016e-fff6-4be9-99ac-255853e298ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 corpus items are [[12, 447, 202, 35, 41, 20, 12, 571, 11, 282, 2, 9, 8, 196, 1, 1549, 8, 201, 71, 123, 654, 319, 4500, 43, 2394, 58, 1408, 1478, 50, 483, 8, 196, 2, 50, 28, 572, 664, 20, 1, 3444, 458, 616, 450, 9, 388, 38, 1, 27, 4501, 53, 178, 664, 25, 1, 1631, 15, 46, 41, 1, 138, 85, 600, 4, 1632, 2, 46, 43, 2217, 2726, 9, 8, 1388, 2, 693, 1, 66, 74, 109, 23, 86, 178, 163, 17, 77, 356, 632, 45, 43, 1036, 2, 2395, 80, 130, 54, 15, 113, 9, 9, 8, 99, 170, 140, 20, 1, 122, 545, 196, 3, 23, 1, 475, 2218, 3230, 770, 1409, 2727, 2, 9, 8, 301, 2, 108, 9, 154, 16, 144, 859, 6, 43, 8190, 243, 16, 8, 99, 2, 9, 364, 123, 1, 179, 998, 9, 8, 1, 66, 812, 74, 109, 23, 750, 3, 142, 139, 5, 48, 64], [3, 19, 69, 730, 273, 62, 107, 187, 197, 351, 52, 14, 27, 9, 731, 5, 610, 15, 15, 59, 551, 272, 17, 22, 305, 8191, 52, 190, 13, 43, 335, 1821, 33, 22, 157, 107, 38, 13, 10, 121, 928, 12, 283, 2, 3, 572, 26, 52, 159, 560, 1367, 14, 524, 633, 9, 8, 178, 897, 73, 97, 3, 320, 11, 4, 633, 715, 2, 320, 18, 67, 19, 5, 139, 1317, 5, 46, 4, 915, 21, 17, 171, 1550, 30, 634, 54, 1, 642, 382, 64, 45, 578, 371, 292, 18, 32, 634, 26, 159, 5688, 2, 1, 512, 154, 2, 94, 50, 239, 898, 272, 8, 44, 826, 45, 1, 1440, 13, 634, 83, 5, 1, 512, 5, 1, 285, 1, 204, 32, 44, 34, 29, 82, 18, 1593, 50, 898, 254, 18, 324, 61, 18, 295, 26, 417, 8192, 18, 1018, 1, 884, 2872, 2873, 2, 1, 245, 1410, 1, 330, 148, 25, 18, 59, 200, 103, 80, 1, 2873, 8, 257, 2, 18, 94, 1, 5044, 42, 885, 2, 94, 1, 245, 840, 148, 200, 32, 174, 12, 283, 672, 1, 148, 138, 2, 3, 672, 1, 2873, 138, 1, 2873, 325, 19, 4, 8193, 180, 21, 253, 101, 3, 38, 12, 180, 18, 23, 5, 860, 352, 6, 1, 148, 5, 129, 9, 165, 2, 18, 32, 39, 1, 361, 68, 417, 5689, 25, 140, 8, 31, 2, 24, 38, 217, 197, 1979, 13, 731, 5, 610, 15, 13, 15, 19, 5, 103, 217, 218, 600, 88, 36, 217, 197, 1979, 19, 62, 1504, 1368], [11066, 11067, 2, 3, 60, 11068, 410, 595, 51, 44, 1677, 2, 5045, 68, 4, 222, 6, 4502, 4, 622, 11069, 2396, 3024, 11070, 2, 4, 2054, 16, 4503, 1, 266, 595, 2, 8194, 4085, 325, 4, 259, 418, 6, 2219, 1, 595, 232, 2, 6697, 15, 59, 166, 2220, 4504, 2, 11071, 540, 53, 11072, 799, 36, 100, 1, 595, 2, 4502, 1, 5690, 10, 194, 7, 257, 5, 296, 1, 740, 513, 751, 2, 11073], [1551, 441, 2494, 11074, 7, 4, 34, 623, 24, 5, 48, 182, 1713, 21, 296, 35, 4086, 15, 37, 15, 19, 121, 1368, 6698, 1099, 999, 16, 2494, 2, 732, 1, 357, 16, 62, 2601, 29, 15, 1287, 55, 928, 2, 344, 30, 655, 37, 15, 79, 411, 39, 546, 1633, 29, 3, 40, 119, 38, 3, 75, 124, 5691, 22, 8195, 51, 101, 18, 6699, 45, 80, 13, 7, 1182, 494, 5, 2494, 2, 158, 174, 93, 818, 94, 4, 284, 11, 517], [1479, 61, 212, 527, 2, 368, 41, 87, 3, 611, 41, 3, 23, 5, 48, 64, 1, 177, 134, 11, 73, 1, 28, 7, 13, 34, 14, 649, 95, 390, 1183, 385, 19, 656, 11075, 11076, 37, 3, 1714, 56, 1079, 153, 4505, 3231, 5, 1164, 3025, 170, 448, 5, 1080, 1764, 6700, 488, 1, 1871, 1411, 733, 1765, 12, 1146, 2, 12, 95, 4506, 2728, 11077, 11, 2602, 4, 131, 27, 5, 103, 9, 388, 11078, 45, 1, 319, 21, 54, 3, 694, 1, 361, 3, 8, 345, 26, 531, 68, 1, 514, 1081, 2, 3026, 321, 3, 126, 185, 11, 136, 5, 48, 1, 122, 8, 174, 3, 288, 1120, 36, 1, 555, 4507, 1262, 3729, 1822, 1822, 977, 190, 1412, 10, 1019, 1594, 9, 123, 9, 1288, 5, 567, 190, 1410, 61, 74, 23, 25, 260, 791, 5046, 424, 4508, 2, 4509, 5047, 5692, 11079, 650, 17, 22, 200, 32, 44, 108, 800, 21, 1, 424, 4508, 5048, 1, 610, 25, 110, 303, 3, 5693, 62, 1823, 45, 12, 11080, 3729, 2, 1822, 1822, 5049, 2603, 293, 4, 1766, 1, 495, 141, 7, 2604, 3, 65, 2605, 53, 3, 8, 822, 5, 103, 1, 1480, 495, 21, 9, 8, 84, 215, 10, 469, 9, 36, 8, 21, 86, 4, 206, 6701, 54, 9, 382, 5, 215, 1262, 1, 2495, 7, 5694, 2, 108, 17, 1505, 3445, 2, 62, 916, 10, 33, 84, 71, 7, 4, 498, 1263, 29, 37, 1, 34, 28, 298, 216, 5, 2055, 35, 100, 1, 801, 10, 14, 92, 11081, 119, 86, 4, 3232, 11, 331, 5050, 801, 2, 8196, 8197, 7, 12, 6702, 485, 4, 4087, 6, 106, 2, 2874, 1481, 100, 1, 495, 141, 51, 99, 36, 1, 4088, 22, 31, 60, 1, 2729]]\n",
            "Length of corpus is 4056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(tokenized_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B_z5Udki-_s",
        "outputId": "abe39b36-7d65-42df-9ee3-829481a3f3d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(corpus, vocab_size, window_size=2, sentence_batch_size=15,  batch_size=250):\n",
        "    np.random.shuffle(np.array(corpus))\n",
        "    number_of_sentence_batches = (len(corpus) // sentence_batch_size) + 1\n",
        "    for batch in range(number_of_sentence_batches):\n",
        "        lower_end = batch*batch_size\n",
        "        upper_end = (batch+1)*batch_size if batch+1 < number_of_sentence_batches else len(corpus)\n",
        "        mini_batch_size = upper_end - lower_end\n",
        "        maxlen = window_size*2\n",
        "        X = []\n",
        "        Y = []\n",
        "        for review_id, words in enumerate(corpus[lower_end:upper_end]):\n",
        "            L = len(words)\n",
        "            for index, word in enumerate(words):\n",
        "                contexts = []\n",
        "                labels   = []            \n",
        "                s = index - window_size\n",
        "                e = index + window_size + 1\n",
        "\n",
        "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
        "                labels.append(word)\n",
        "\n",
        "                x = pad_sequences(contexts, maxlen=maxlen)\n",
        "                y = np_utils.to_categorical(labels, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "        X = tf.constant(X)\n",
        "        Y = tf.constant(Y)\n",
        "        number_of_batches = len(X) // batch_size\n",
        "        for real_batch in range(number_of_batches):\n",
        "          lower_end = batch*batch_size\n",
        "          upper_end = (batch+1)*batch_size\n",
        "          batch_X = tf.squeeze(X[lower_end:upper_end])\n",
        "          batch_Y = tf.squeeze(Y[lower_end:upper_end])\n",
        "          yield (batch_X, batch_Y)\n"
      ],
      "metadata": {
        "id": "B_Z1eJZrhK7K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=4))\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)))\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "cbow.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHtu75Kpi6XF",
        "outputId": "3d6267f8-6d9b-4ae0-f126-dcb4f4143b14"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 50)             997500    \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 50)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 19950)             1017450   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,014,950\n",
            "Trainable params: 2,014,950\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model():\n",
        "    if not BATCH:\n",
        "        X, Y = generate_data(corpus=tokenized_corpus, vocab_size=vocab_size, batch_size=len(X))\n",
        "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
        "        cbow.fit(X, Y, epochs = epochs)\n",
        "    else:\n",
        "        index = 1\n",
        "        for x, y in generate_data(corpus=tokenized_corpus, vocab_size=vocab_size, batch_size=batch_size):\n",
        "            print(f'Training on Iteration: {index}')\n",
        "            index += 1\n",
        "            history = cbow.train_on_batch(x, y, reset_metrics=False, return_dict=True)\n",
        "            print(history)\n",
        "            if index > epochs:\n",
        "                break"
      ],
      "metadata": {
        "id": "g44ICdUcj7ZL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTM2wqbzke5n",
        "outputId": "5b0387ef-bdee-45fc-9449-fa7109413ec1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Iteration: 1\n",
            "{'loss': 9.901032447814941}\n",
            "Training on Iteration: 2\n",
            "{'loss': 9.89981460571289}\n",
            "Training on Iteration: 3\n",
            "{'loss': 9.89858341217041}\n",
            "Training on Iteration: 4\n",
            "{'loss': 9.89734172821045}\n",
            "Training on Iteration: 5\n",
            "{'loss': 9.89609146118164}\n",
            "Training on Iteration: 6\n",
            "{'loss': 9.894830703735352}\n",
            "Training on Iteration: 7\n",
            "{'loss': 9.893559455871582}\n",
            "Training on Iteration: 8\n",
            "{'loss': 9.8922758102417}\n",
            "Training on Iteration: 9\n",
            "{'loss': 9.89097785949707}\n",
            "Training on Iteration: 10\n",
            "{'loss': 9.889666557312012}\n",
            "Training on Iteration: 11\n",
            "{'loss': 9.888338088989258}\n",
            "Training on Iteration: 12\n",
            "{'loss': 9.886992454528809}\n",
            "Training on Iteration: 13\n",
            "{'loss': 9.885626792907715}\n",
            "Training on Iteration: 14\n",
            "{'loss': 9.884241104125977}\n",
            "Training on Iteration: 15\n",
            "{'loss': 9.882833480834961}\n",
            "Training on Iteration: 16\n",
            "{'loss': 9.881401062011719}\n",
            "Training on Iteration: 17\n",
            "{'loss': 9.879944801330566}\n",
            "Training on Iteration: 18\n",
            "{'loss': 9.878460884094238}\n",
            "Training on Iteration: 19\n",
            "{'loss': 9.876949310302734}\n",
            "Training on Iteration: 20\n",
            "{'loss': 9.875406265258789}\n",
            "Training on Iteration: 21\n",
            "{'loss': 9.873831748962402}\n",
            "Training on Iteration: 22\n",
            "{'loss': 9.872223854064941}\n",
            "Training on Iteration: 23\n",
            "{'loss': 9.870579719543457}\n",
            "Training on Iteration: 24\n",
            "{'loss': 9.86889934539795}\n",
            "Training on Iteration: 25\n",
            "{'loss': 9.867179870605469}\n",
            "Training on Iteration: 26\n",
            "{'loss': 9.865419387817383}\n",
            "Training on Iteration: 27\n",
            "{'loss': 9.863615989685059}\n",
            "Training on Iteration: 28\n",
            "{'loss': 9.861766815185547}\n",
            "Training on Iteration: 29\n",
            "{'loss': 9.859870910644531}\n",
            "Training on Iteration: 30\n",
            "{'loss': 9.857926368713379}\n",
            "Training on Iteration: 31\n",
            "{'loss': 9.85593032836914}\n",
            "Training on Iteration: 32\n",
            "{'loss': 9.8538818359375}\n",
            "Training on Iteration: 33\n",
            "{'loss': 9.851777076721191}\n",
            "Training on Iteration: 34\n",
            "{'loss': 9.849616050720215}\n",
            "Training on Iteration: 35\n",
            "{'loss': 9.847394943237305}\n",
            "Training on Iteration: 36\n",
            "{'loss': 9.845111846923828}\n",
            "Training on Iteration: 37\n",
            "{'loss': 9.842765808105469}\n",
            "Training on Iteration: 38\n",
            "{'loss': 9.840353965759277}\n",
            "Training on Iteration: 39\n",
            "{'loss': 9.837873458862305}\n",
            "Training on Iteration: 40\n",
            "{'loss': 9.835322380065918}\n",
            "Training on Iteration: 41\n",
            "{'loss': 9.8326997756958}\n",
            "Training on Iteration: 42\n",
            "{'loss': 9.830001831054688}\n",
            "Training on Iteration: 43\n",
            "{'loss': 9.827227592468262}\n",
            "Training on Iteration: 44\n",
            "{'loss': 9.824374198913574}\n",
            "Training on Iteration: 45\n",
            "{'loss': 9.821439743041992}\n",
            "Training on Iteration: 46\n",
            "{'loss': 9.818422317504883}\n",
            "Training on Iteration: 47\n",
            "{'loss': 9.81531810760498}\n",
            "Training on Iteration: 48\n",
            "{'loss': 9.812128067016602}\n",
            "Training on Iteration: 49\n",
            "{'loss': 9.808846473693848}\n",
            "Training on Iteration: 50\n",
            "{'loss': 9.805474281311035}\n",
            "Training on Iteration: 51\n",
            "{'loss': 9.802007675170898}\n",
            "Training on Iteration: 52\n",
            "{'loss': 9.798445701599121}\n",
            "Training on Iteration: 53\n",
            "{'loss': 9.794785499572754}\n",
            "Training on Iteration: 54\n",
            "{'loss': 9.791025161743164}\n",
            "Training on Iteration: 55\n",
            "{'loss': 9.787163734436035}\n",
            "Training on Iteration: 56\n",
            "{'loss': 9.783197402954102}\n",
            "Training on Iteration: 57\n",
            "{'loss': 9.779126167297363}\n",
            "Training on Iteration: 58\n",
            "{'loss': 9.774947166442871}\n",
            "Training on Iteration: 59\n",
            "{'loss': 9.770657539367676}\n",
            "Training on Iteration: 60\n",
            "{'loss': 9.766257286071777}\n",
            "Training on Iteration: 61\n",
            "{'loss': 9.761743545532227}\n",
            "Training on Iteration: 62\n",
            "{'loss': 9.757116317749023}\n",
            "Training on Iteration: 63\n",
            "{'loss': 9.752370834350586}\n",
            "Training on Iteration: 64\n",
            "{'loss': 9.74750804901123}\n",
            "Training on Iteration: 65\n",
            "{'loss': 9.742524147033691}\n",
            "Training on Iteration: 66\n",
            "{'loss': 9.737418174743652}\n",
            "Training on Iteration: 67\n",
            "{'loss': 9.732190132141113}\n",
            "Training on Iteration: 68\n",
            "{'loss': 9.726836204528809}\n",
            "Training on Iteration: 69\n",
            "{'loss': 9.721357345581055}\n",
            "Training on Iteration: 70\n",
            "{'loss': 9.71574878692627}\n",
            "Training on Iteration: 71\n",
            "{'loss': 9.710012435913086}\n",
            "Training on Iteration: 72\n",
            "{'loss': 9.704144477844238}\n",
            "Training on Iteration: 73\n",
            "{'loss': 9.69814395904541}\n",
            "Training on Iteration: 74\n",
            "{'loss': 9.692009925842285}\n",
            "Training on Iteration: 75\n",
            "{'loss': 9.685741424560547}\n",
            "Training on Iteration: 76\n",
            "{'loss': 9.679336547851562}\n",
            "Training on Iteration: 77\n",
            "{'loss': 9.6727933883667}\n",
            "Training on Iteration: 78\n",
            "{'loss': 9.666112899780273}\n",
            "Training on Iteration: 79\n",
            "{'loss': 9.659292221069336}\n",
            "Training on Iteration: 80\n",
            "{'loss': 9.652329444885254}\n",
            "Training on Iteration: 81\n",
            "{'loss': 9.64522647857666}\n",
            "Training on Iteration: 82\n",
            "{'loss': 9.637978553771973}\n",
            "Training on Iteration: 83\n",
            "{'loss': 9.630587577819824}\n",
            "Training on Iteration: 84\n",
            "{'loss': 9.623050689697266}\n",
            "Training on Iteration: 85\n",
            "{'loss': 9.615368843078613}\n",
            "Training on Iteration: 86\n",
            "{'loss': 9.607539176940918}\n",
            "Training on Iteration: 87\n",
            "{'loss': 9.59956169128418}\n",
            "Training on Iteration: 88\n",
            "{'loss': 9.591436386108398}\n",
            "Training on Iteration: 89\n",
            "{'loss': 9.583160400390625}\n",
            "Training on Iteration: 90\n",
            "{'loss': 9.574735641479492}\n",
            "Training on Iteration: 91\n",
            "{'loss': 9.56615924835205}\n",
            "Training on Iteration: 92\n",
            "{'loss': 9.5574312210083}\n",
            "Training on Iteration: 93\n",
            "{'loss': 9.548551559448242}\n",
            "Training on Iteration: 94\n",
            "{'loss': 9.539519309997559}\n",
            "Training on Iteration: 95\n",
            "{'loss': 9.530333518981934}\n",
            "Training on Iteration: 96\n",
            "{'loss': 9.520994186401367}\n",
            "Training on Iteration: 97\n",
            "{'loss': 9.51150131225586}\n",
            "Training on Iteration: 98\n",
            "{'loss': 9.501853942871094}\n",
            "Training on Iteration: 99\n",
            "{'loss': 9.49205207824707}\n",
            "Training on Iteration: 100\n",
            "{'loss': 9.482094764709473}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./cbow_scratch_synonims.txt' ,'w') as f:\n",
        "    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
        "    vectors = cbow.get_weights()[0]\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
        "        f.write('{} {}\\n'.format(word, str_vec))"
      ],
      "metadata": {
        "id": "GR97HVOqkoMI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./cbow_scratch_synonims.txt', binary=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "SvMp9eWsk2Z-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(positive=['gasoline'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0dw_S7Kk6lW",
        "outputId": "ce29f09a-b1b5-4cc1-d432-3963bde9cf3d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('maintains', 0.5357844233512878),\n",
              " ('96', 0.5284336805343628),\n",
              " (\"there're\", 0.5171193480491638),\n",
              " ('chewing', 0.5127679109573364),\n",
              " ('crisscrosses', 0.496882826089859),\n",
              " ('funsports', 0.48369425535202026),\n",
              " ('biancoverde', 0.48104915022850037),\n",
              " ('rewards', 0.478218138217926),\n",
              " ('surf', 0.47760260105133057),\n",
              " ('chairs', 0.47078537940979004)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(negative=['apple'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCmSyCj8k6He",
        "outputId": "99788923-e65b-4984-802d-fca4d0fa87b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rowdy', 0.519658088684082),\n",
              " ('glom', 0.5132775902748108),\n",
              " ('bedroom', 0.49135029315948486),\n",
              " ('refresher', 0.4903404712677002),\n",
              " ('shirt', 0.4868806004524231),\n",
              " ('dali', 0.4821418821811676),\n",
              " ('yankin', 0.4679061472415924),\n",
              " ('clip', 0.46546414494514465),\n",
              " ('flakey', 0.4637853801250458),\n",
              " ('grandpa', 0.4583456218242645)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjvrCdY5lkJk"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}